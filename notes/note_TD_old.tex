\documentclass[a4paper]{article}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx,wrapfig,lipsum}

\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}






\usepackage{enumitem}
\newcommand{\subscript}[2]{$#1 _ #2$}
\usepackage{listings}
 \usepackage{xcolor}



\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=red,
    pdftitle={An Example},
    pdfpagemode=FullScreen,
    }
\urlstyle{same}


\def\ba{\begin{eqnarray}}
\def\ea{\end{eqnarray}}


\title{ Euclid summer school: Parameter estimation for a polynomial model}

\begin{document}

\maketitle


\section{Analytical solution}

   \begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\columnwidth]{data_example.png}
  \caption{The measured data points as a function of redshift}
  \label{fig:data}
\end{figure}


We have access to N measurements of a physical phenomenon that evolves as a function of redshift. Our data model for this phenomenon is an order M polynomial 
\ba
d(z_{i}) = d_{i} =  \sum_{k=0}^{M} a_{k}(z_{i}-z_{c})^{k} + n(z_{i})
\ea
$a_{k}$ are the model parameters that we would like to estimate, $z_i$ are the N redshifts at which the measurements have been done and  $n(z_{i})=n_{i}$ is the noise on the measurements which follows a multivariate gaussian distribution ${\cal N}(0,\Sigma)$. \\
We will assume for now that M=3 and $z_{c}=0$. \\


1) Give an analytic formula for the likelihood of the data given the model parameters ${\cal L}( \{d_{i} \} | \{a_{k} \}) $ (think about the probability distribution  followed by $d_{i} - \sum_{k=0}^{M} a_{k}  (z_{i}-z_{c})^{k}$ \\


2) Download the data file: data\_example.txt, 
 in  \href{https://github.com/thibautlouis/TD/}{here}, the column are redshift, data, error, and reproduce Figure 1. \\


3) For convenience, we will write the data model in vectorial form ${\bm d} = {\cal P}\bm{a} + {\bm n}$   where 
\ba
 {\cal P}= 
 \begin{pmatrix} 
1 &
(z_{0}-z_{c}) & 
(z_{0} -z_{c})^{2}& 
... & 
(z_{0} -z_{c})^{M}
\cr
1 &
(z_{1}-z_{c}) & 
(z_{1} -z_{c})^{2}& 
... & 
(z_{1} -z_{c})^{M}
\cr
... & 
...  &
... & 
... & 
...& 
\cr
1 &
(z_{N-1}-z_{c}) & 
(z_{N-1} -z_{c})^{2}& 
... & 
(z_{N-1} -z_{c})^{M}
\cr
\end{pmatrix}, \
 {\bm a}= 
 \begin{pmatrix} 
a_{0} \cr
a_{1} \cr
... \cr
a_{M} \cr
\end{pmatrix}
\ea



$ {\cal P}$ is a (N $\times$ (M+1)) matrix, write a function that build the matrix ${\cal P}$ using the redshift column of the data file. \\

4) The variance of the noise on the measurement is given by $ \Sigma_{ii} =  \sigma_{0}^{2} + \sigma_{1}^{2} (1+z_{i})^{3}$, with $\sigma_{0} = \sqrt{3}$ and  $\sigma_{1} = 1$   the correlation between the different measurements is null but for adjacent redshifts, in that case it takes the constant value of 0.1

\ba
\rho_{ij} = \Sigma_{ij}/\sqrt{\Sigma_{ii}\Sigma_{jj}}= 0.1& \mbox{only if} & |i-j|=1 
\ea

Write a function that construct the covariance matrix, check that the square root of its diagonal do agree with the error column of the data file, then evaluate the $\chi^{2} = ({\bm d}  - {\cal P}\bm{a})^{T} \Sigma^{-1} ({\bm d}  - {\cal P}\bm{a})$ for the set of parameters $a_{0}= 8$, $a_{1}=3$, $a_{2}=0.4$, $a_{3} = -0.02$,  you should get $\chi^{2} \sim 122.9827$, is this set of parameter a good fit to the data ? \\

5) in our model, the different parameters $a_{k}$ enter linearly in the model,  we can analytically derive the Maximum Likelihood (ML) solution, write down an expression for $\ln({\cal L}({\bm d} |  {\bm a}))$, and show that its extremum is obtained when 
\ba
   \bm{\hat{a}} = ( {\cal P}^{T}  \Sigma^{-1}{\cal P})^{-1}{\cal P}^{T}  \Sigma^{-1} \bm{d} 
\ea
tip: ask for $ \nabla_{ \bm{a} } {\rm ln}{\cal L}(\bm{d} | \bm{a})=0 |_{    \bm{a} =   \bm{\hat{a}}}$
This solution is called the Maximum Likelihood estimate of $\bm{a} $ and is denoted $\bm{\hat{a}}$. \\

6) Show (analytically) that the expectation value of the ML estimator and its covariance are given by

\ba
\langle  \bm{\hat{a}}  \rangle= \bm{a},  \ \ \
{\rm Cov}(\bm{\hat{a}}) &=&  ( {\cal P}^{T}  \Sigma^{-1}{\cal P})^{-1}
\ea

7) Write a function that evaluate the maximum solution corresponding to the provided data file, print the value of the  $\hat{a}_{k}$ parameters and their associated uncertainties. \\

8) Plot the best fit model with respect to the data point, estimate the  $\chi^{2} = ({\bm d}  - {\cal P}\bm{\hat{a}})^{T} \Sigma^{-1} ({\bm d}  - {\cal P}\bm{\hat{a}})$ and p-value of the best fit model. \\

9) Compute the correlation matrix associated with ${\rm Cov}(\bm{\hat{a}}) $ and plot it, you should see that the parameters are pretty correlated.

\subsection*{Optional}
Let's display confidence ellipses, a bunch of ellipses representing the uncertainties associated to each pairs of parameter, first we need to write the marginal probability distribution function associated to two of the parameters.
\ba
{\cal P}(a_{1}, a_{3}) &=& \int da_{0} da_{2}  {\cal P} (a_{0}, a_{1},  a_{2}, a_{3}) \\
\ea
For a multivariate gaussian distribution,  the marginal is still  a multivariate gaussian distribution just "removing" variables we have marginalized over 

\ba
{\cal P}(a_{1}, a_{3}) = \frac{1}{2\pi \sqrt{\rm  det \ \tilde{\Sigma}} } {\rm exp}  - \frac{1}{2} \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix}
 \right]^{T} \tilde{\Sigma}^{-1}  \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix} \right]   
\ea

\ba
\tilde{\Sigma}= 
 \begin{pmatrix} 
\Sigma_{a_{1}a_{1}} &
\Sigma_{a_{1}a_{3}} 
\cr
\Sigma_{a_{3}a_{1}}  &
\Sigma_{a_{3}a_{3}} 
\cr
\end{pmatrix}
\ea

Then let's say we want to display the 68$\%$ confidence ellipse, it is a contour that englobes each pairs of parameters ($a_{1}$, $a_{3}$)  which have an associated probability $>32 \%$.
To do so we use the fact that   $ \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix}
 \right]^{T} \tilde{\Sigma}^{-1}  \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix} \right]    $ follow a $\chi^{2}$ distribution with two degree of freedom, the question become what is the value y for which the probability $P(\chi^{2}_{2\rm DoF}<y) = 68\%$. \\

10)  Use the percent point function (also called quantile) of the $\chi^{2}$ distribution with 2 DoF (e.g from scipy.stats) to find the value of y. \\

The contour of the confidence ellipse would therefore be given by the following equation
\ba
 \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix}
 \right]^{T} \tilde{\Sigma}^{-1}  \left[  \begin{pmatrix} 
a_{1} - \hat{a}_{1} \cr
a_{3} - \hat{a}_{3} \cr
\end{pmatrix} \right]    = y
\ea
We can diagonalize the matrix
\ba
{\bm \Delta a}^{T} P D^{-1}Â P^{T} {\bm  \Delta a}    = y \\
{\bm b} = P^{T}  {\bm  \Delta a} \\
{\bm b}^{T} D^{-1} {\bm  b} = y \\
\sum_{i=1,2} b^{2}_{i}/ \lambda_{i}y = 1
\ea
Because   P is an orthogonal matrix (a rotation), we can see that this is the equation of a standard ellipse when using the eigenvectors as the coordinate system

The center of the ellipse is given by $\bm{\hat{a}}$.
The direction of the semi-major (semi-minor) axis of the ellipse is given by the  eigenvector associated with the largest (smallest) eigenvalues, and their lenght is given by $\sqrt{\lambda_{i}y}$ \\

11) Plot the confidence ellipse for each $(a_{i}, a_{j})$ pairs. \\

12) Set $z_{c}$ different from zero and re-estimate the maximum likelihood parameters, how does the best fit $\chi^{2}$ of the data with respect to the model change ? how does the correlation matrix between parameters change ?
 



  % \begin{figure}[h!]
 % \centering
 % \includegraphics[width=0.8\columnwidth]{musk.png}
 % \caption{Negociations between Elon musk and Nasa for building a new satellite}
 % \label{fig:data}
%\end{figure}

\section{Monte Carlo verification of the estimator}

A Monte Carlo method designs a numerical method using random processes to estimate numerical quantities. \\

We have written an analytic formula for the maximum likelihood solution to our parameter estimation problem, here we would like to check numerically that the estimator is indeed unbiased and that the analytic covariance formula is indeed correct \\
We will draw simulations of the data,  there is two steps: 1.  Perform a Choleski decomposition of the noise covariance matrix $\Sigma = LL^{T}$, 2. Generate vectors of random gaussian  number, ${\bm u}$, of size N with mean 0 and covariance $\delta_{ij}$. 
A noise simulation will be given by: ${\bm n}^{simu}= L {\bm u} $ \\

1)  Choose a set of parameters $\bm{a}$ and generate a bunch of simulation of the data 

\ba
{\bm d}_{\ \rm sim \ 1} &=& {\cal P} \bm{a} + {\bm n}^{\rm sim 1} \\
&....& \nonumber \\
&....& \nonumber  \\
{\bm d}_{\ \rm sim \  1000} &=& {\cal P}\bm{a} + {\bm n}^{\rm sim 1000}
\ea

for each simulation estimate the maximum solution for  $\bm{a}$, create a list of estimated $\{ \bm{\hat{a}}_{\rm sim 1}, \bm{\hat{a}}_{\rm sim 2}, ...., \bm{\hat{a}}_{\rm sim 1000} \}$ \\

2) Compute the numerical mean and covariances of the list of estimated maximum likelihood parameters, does the mean match your input parameters ? does the numerical covariance matches your analytical formula ? \\

3) For each simulation compute the associated $\chi^{2}$ e.g,  $\chi^{2}_{\rm sim 1 } = ({\bm d}_{\rm sim 1}  - {\cal P}\bm{\hat{a}_{\rm sim 1}})^{T} \Sigma^{-1} ({\bm d}_{\rm sim 1}  - {\cal P}\bm{\hat{a}_{\rm sim 1}})$ and p-value.  \\

4) Display the $\chi^{2}$ distribution of the simulations (plot an histogram with the values of $\{\chi^{2}_{\rm sim 1 }, ..., \chi^{2}_{\rm sim 1000  } \}$ and compare with the expected distribution of $\chi^{2}$ with N-M DoF, do the same with the p-value, what is the expected distribution of p-value ?

% $\nabla_{\bm{a}) \ln({\cal L}(d | \{a_{k} \})$

 \section{Monte Carlo Markov chains}

We have been able to solve this problem analytically, this is due to the fact that the parameters enter linearly in the model, for a more general case we will have to use different methods.
A standard method is Monte Carlo Markov chains, the idea is to design an algorithm that draw samples from the posterior distribution of the parameters. In this section we will code our own MCMC algorithm and redo the parameter estimation of our model. \\

A popular MCMC algorithm is the Metropolis Hasting algorithm, schematically

\begin{enumerate}[label=(\subscript{I}{{\arabic*}})]
\item Define a function $f(\bm{a}) =  {\cal L}({\bm d} |  {\bm a})  P(\bm a)$ \\
\item Choose a starting point for your parameters $\bm a_{0}$ \\
\item Choose a proposal $g({\bm a'}| {\bm a_{0}})$, it is the function that will help us move in the parameter space, a usual choice is a multivariate distribution centered on $\bm a_{0}$ with covariance $ \Sigma=  \frac{2.4^{2}}{D} \rm Cov({\bm{a})}$
\ba
g({\bm a'}| {\bm a_{0}}) = \frac{1}{(2\pi)^{D/2} \sqrt{|\Sigma |}} {\rm exp}  - \frac{1}{2} \left[   
({\bm a'} - {\bm a_{0}})
 \right]^{T} \Sigma^{-1}  \left[   
({\bm a'} - {\bm a_{0}})
 \right]
\ea
\item Randomly draw a candidate ${\bm a'}$ from the proposal
\item Compute the acceptance rate $\alpha = \frac{f(\bm{a'})}{f(\bm{a_{0}})}$ 
\item Draw a uniform random number between 0 and 1 
\ba
u \leq \alpha & \mbox{accept the candidate} & \bm{a_{1}} = {\bm a'}  \\
u > \alpha  & \mbox{reject the candidate} & \bm{a_{1}} = \bm{a_{0}}
\ea
\item Iterate \\

The idea of the algorithm is the following, we explore the parameter space, if we move toward a  state with higher probability, the move is always accepted.
If we move towards a less probable state, the move can be either accepted or rejected, the rejection depends on the ratio of probability between the two points in parameter space. The random walk preferentially explores the parameter space where the posterior is high but occasionally moves in region with lower probability, this helps avoiding being trapped in local maxima.  \\
 



1) Write down a version of the Metropolis Hasting algorithm, and draw $10^{4}$ samples of $\bm{a}$ . \\

2) Plot the corresponding chains. At the beginning of the chains we will notice a "burn in" phase, a phase where parameters value varies by a lot, this should be discarded for later analysis. \\

3) Plot an histogram of the chains, this represents the posterior distribution of our estimated parameters, compare this with the analytical estimate.

4) In practice you won't have to use your own MCMC sampler since many of them are already publicly available, a popular one in cosmology is cobaya an example of how to use cobaya is given below:
 
 

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    language=python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=1
}

\lstset{style=mystyle}



 \begin{lstlisting}
 def cobaya_mcmc(z, data, inv_data_cov, z_c, min_list, max_list, 
 			      chain_name, Rminus1_stop=0.003, Rminus1_cl_stop=0.05):

    from cobaya.run import run

    def log_prob(a0, a1, a2, a3):
        params = [a0, a1, a2, a3]
        model = fp1.generate_model(z, params, z_c)
        res = data - model
        return -0.5 * res @ inv_data_cov @ res

    info = {
        "likelihood": {"my_like": log_prob},
        "params": {
            "a0": {"prior": {"min": min_list[0], "max": max_list[0]}, "latex": "a_{0}"},
            "a1": {"prior": {"min": min_list[1], "max": max_list[1]}, "latex": "a_{1}"},
            "a2": {"prior": {"min": min_list[2], "max": max_list[2]}, "latex": "a_{2}"},
            "a3": {"prior": {"min": min_list[3], "max": max_list[3]}, "latex": "a_{3}"},
        },
        "sampler": {
            "mcmc": {
                "max_tries": 10 ** 8,
                "Rminus1_stop": Rminus1_stop,
                "Rminus1_cl_stop": Rminus1_cl_stop,
            }
        },
        "output": f"{chain_name}",
        "force": True,
        "debug": False,
    }
    

    updated_info, sampler = run(info)
\end{lstlisting}

you can then plot the result using getDist 
 
  \begin{lstlisting}

 def plot_cobaya_chain(chain_name, params):
    from getdist.mcsamples import loadMCSamples
    import getdist.plots as gdplt

    samples = loadMCSamples( f"{chain_name}", settings = {"ignore_rows": 0.5})
    gdplot = gdplt.get_subplot_plotter()
    gdplot.triangle_plot(samples, params, filled = True, title_limit=1)
    plt.savefig(f"{chain_name}.png", dpi = 300)
    plt.clf()
    plt.close()
\end{lstlisting}
\end{enumerate}

\section{New data, new physics?}

\begin{minipage}[b]{0.5\linewidth}
After some sharp negociation with Elon Musk, Nasa manages to fund another experiment to measure our observable. The data file, data\_example\_precise.txt can be found in \href{https://github.com/thibautlouis/TD/}{here} , The variance of the noise on the measurement is still given by $ \Sigma_{ii} =  \sigma_{0}^{2} + \sigma_{1}^{2} (1+z_{i})^{3}$, but now $\sigma_{0} = \sqrt{3}/5$ and  $\sigma_{1} = 1/5$   the correlation between the different measurements is null but for adjacent redshifts, in that case it takes the constant value of 0.1
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\linewidth}
\includegraphics[height=22\baselineskip]{musk}
\end{minipage} \\

1) Plot the data file on the top of the previous one, are the two data set consistent with each other ? \\

2) Redo the analytic fit using this data set and compute the $\chi^{2}$ and associated p-value, what should we conclude ? \\

Reys et al proposed a simple modification to the standard model that could explain the new data with the following lagrangian
\begin{align}
\label{eq:L-corr}
e^{-1}\mathcal{L}_{4\partial}^{\mathrm{P}} =&\; \Bigl(\frac{1}{\kappa^2} + \frac{1}{2\sqrt{3}}\,(5\,c_1 + 24\,c_2)\,g^2\Bigr)R -\frac14\,\Bigl(\frac{1}{\kappa^2} + \frac{7}{6\sqrt{3}}\,(5\,c_1 - 12\,c_2)\,g^2\Bigr)F_{ab}^2 \nonumber \\[1mm]
&+ 12\,\Bigl(\frac{1}{\kappa^2} + \frac{1}{12\sqrt{3}}\,(25\,c_1 + 156\,c_2)\,g^2\Bigr)g^2 \nonumber \\[1mm] 
&+ \frac{1}{12\sqrt{3}}\,\Bigl(\frac{1}{\kappa^2} - \frac{3\sqrt{3}}{2}\,(c_1 + 6\,c_2)\,g^2\Bigr)e^{-1}\varepsilon^{\mu\nu\rho\sigma\tau}W_\mu F_{\nu\rho}F_{\sigma\tau}  \nonumber \\[1mm]
&+ \frac{1}{24\sqrt{3}}(2\,c_1 - 3\,c_2)R\,F_{ab}^2 - \frac{5}{4\sqrt{3}}\,c_1\,R^{ab}F_{ac}F_b{}^c + \frac{\sqrt{3}}{16}\,c_1\,R_{abcd}F^{ab}F^{cd} \nonumber \\[1mm]
&+ \frac{1}{16}\,c_1\,e^{-1}\varepsilon^{\mu\nu\rho\sigma\tau}W_\mu R_{\nu\rho}{}^{\lambda\epsilon}R_{\sigma\tau\lambda\epsilon} \nonumber \\[1mm]
&+ \frac{1}{8\sqrt{3}}\,(c_1 + 6\,c_2)R^2 - \frac{1}{2\sqrt{3}}\,c_1\,R_{ab}^2 + \frac{\sqrt{3}}{8}\,c_1\,(R_{abcd})^2 \\[1mm]
&+ \frac{5\sqrt{3}}{64}\,c_1\,F^{ab}F_{a}{}^cF_b{}^d F_{cd} - \frac{1}{1152\sqrt{3}}\,(61\,c_1 - 6\,c_2)\,F_{ab}^2\,F_{cd}^2 \nonumber \\[1mm] 
& - \frac{\sqrt{3}}{2}\,c_1\,(\nabla_a F_{bc})(\nabla^{[a}F^{b]c}) - \frac{\sqrt{3}}{2}\,c_1\,F_{ab}\nabla^b\nabla_c F^{ac} \nonumber \\[1mm] 
&+ \frac{1}{8}\,c_1\,e^{-1}\varepsilon^{\mu\nu\rho\sigma\tau} F_\mu{}^\lambda F_{\sigma\tau}\Bigl(\frac32\,\nabla_\nu F_{\lambda\rho} - \nabla_\lambda F_{\nu\rho}\Bigr) + \frac{3}{32}\,c_1\,e^{-1}\varepsilon^{\mu\nu\rho\sigma\tau} F_{\mu\nu}F_{\rho\sigma}\nabla^\lambda F_{\lambda\tau} + \mathcal{O}(c_i^2) \, . \nonumber
\end{align}

After staring at it for at bit, the community decided that it should roughly, maybe... change  the data model in the following way
\ba
d(z_{i}) = d_{i} =  \sum_{k=0}^{M} a_{k}(z_{i}-z_{c})^{k} \exp[-(\alpha z_{i})^{3}] + n(z_{i})
\ea

3) Redo the MCMC fit now also including $\alpha$ as a parameter, compare the $\chi^{2}$ of the two different models (with and without $\alpha=0$) compute the associated $\Delta \rm AIC$.







\end{document}